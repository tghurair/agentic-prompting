{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from crewai_tools import BaseTool, tool\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Get the API key from the environment\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",  \n",
    "    api_key=api_key  \n",
    ")\n",
    "\n",
    "# Define the agent with prompt engineering capabilities\n",
    "prompt_engineer = Agent(\n",
    "    role='Prompt Engineer',\n",
    "    goal='Enhance and optimize prompts iteratively to improve LLM performance.',\n",
    "    backstory=(\n",
    "        \"You are a seasoned Prompt Engineer with deep expertise in natural language processing. \"\n",
    "        \"Your primary skill is in fine-tuning prompts to maximize clarity, relevance, and effectiveness. \"\n",
    "        \"You have a systematic approach to testing and refining prompts to achieve the best outcomes.\"\n",
    "    ),\n",
    "    memory=True,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "@tool(\"General Prompt\")\n",
    "def general_prompt(idea: str) -> str:\n",
    "    \"\"\"Generate a general prompt based on the given idea.\"\"\"\n",
    "    return f\"Please provide information about {idea}.\"\n",
    "\n",
    "@tool(\"No-Shot Prompt\")\n",
    "def no_shot_prompt(idea: str) -> str:\n",
    "    \"\"\"Generate a no-shot prompt based on the given idea.\"\"\"\n",
    "    return f\"Without any examples, please explain or answer: {idea}\"\n",
    "\n",
    "@tool(\"Few-Shot Prompt\")\n",
    "def few_shot_prompt(idea: str) -> str:\n",
    "    \"\"\"Generate a few-shot prompt based on the given idea.\"\"\"\n",
    "    examples = [\n",
    "        \"Example 1: [Input] -> [Output]\",\n",
    "        \"Example 2: [Input] -> [Output]\",\n",
    "        \"Example 3: [Input] -> [Output]\"\n",
    "    ]\n",
    "    return f\"Given the following examples:\\n{'\\n'.join(examples)}\\n\\nNow, please address: {idea}\"\n",
    "\n",
    "@tool(\"Include-Exclude Prompt\")\n",
    "def include_exclude_prompt(idea: str) -> str:\n",
    "    \"\"\"Generate an include-exclude prompt based on the given idea.\"\"\"\n",
    "    return f\"\"\"\n",
    "    Regarding {idea}:\n",
    "    Include:\n",
    "    - Relevant point 1\n",
    "    - Relevant point 2\n",
    "    Exclude:\n",
    "    - Irrelevant point 1\n",
    "    - Irrelevant point 2\n",
    "    Please provide your response based on these guidelines.\n",
    "    \"\"\"\n",
    "\n",
    "@tool(\"Chain-of-Thought Prompt\")\n",
    "def chain_of_thought_prompt(idea: str) -> str:\n",
    "    \"\"\"Generate a chain-of-thought prompt based on the given idea.\"\"\"\n",
    "    return f\"\"\"\n",
    "    Let's approach {idea} step by step:\n",
    "    1. First, consider...\n",
    "    2. Then, analyze...\n",
    "    3. Next, evaluate...\n",
    "    4. Finally, conclude...\n",
    "    Please provide your reasoning for each step.\n",
    "    \"\"\"\n",
    "\n",
    "@tool(\"CoT Reflection Prompt\")\n",
    "def cot_reflection_prompt(idea: str) -> str:\n",
    "    \"\"\"Generate a chain-of-thought prompt with reflection based on the given idea.\"\"\"\n",
    "    return f\"\"\"\n",
    "    Let's approach {idea} using a reflective chain of thought:\n",
    "    1. Initial thoughts: ...\n",
    "    2. Deeper analysis: ...\n",
    "    3. Potential issues or contradictions: ...\n",
    "    4. Revised perspective: ...\n",
    "    5. Final conclusion: ...\n",
    "    At each step, reflect on your reasoning and consider alternative viewpoints.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tasks\n",
    "analyze_prompt_task = Task(\n",
    "    description=(\n",
    "        \"Analyze the given prompt and choose the most suitable prompt technique for the task. \"\n",
    "        \"Consider aspects such as clarity, relevance, and potential ambiguities. \"\n",
    "        \"Provide a detailed explanation for each identified issue, referencing best practices in prompt engineering. \"\n",
    "        \"Recommend the most appropriate prompt technique (e.g., Chain-of-Thought, Include-Exclude) and justify your choice.\\n\\n\"\n",
    "        \"Prompt: {prompt}\\n\\n\"\n",
    "        \"Your analysis should be thorough and actionable, offering specific suggestions for enhancing the prompt and selecting the best technique.\"\n",
    "    ),\n",
    "    expected_output='A comprehensive analysis of the prompt with detailed explanations, actionable suggestions for improvement, and a recommendation for the most suitable prompt technique.',\n",
    "    agent=prompt_engineer\n",
    ")\n",
    "\n",
    "generate_prompt_task = Task(\n",
    "    description=(\n",
    "        \"Generate a detailed and comprehensive prompt based on the analysis done, ensuring clarity, relevance, and adherence to the guidelines provided.\"\n",
    "    ),\n",
    "    expected_output='prompt that follows the decision made for which prompt technique to use',\n",
    "    agent=prompt_engineer,\n",
    "    tools=[general_prompt , no_shot_prompt , few_shot_prompt , include_exclude_prompt , chain_of_thought_prompt , cot_reflection_prompt]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 15:09:09,740 - 8267676352 - __init__.py-__init__:538 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "crew = Crew(\n",
    "    agents=[prompt_engineer],\n",
    "    tasks=[analyze_prompt_task, generate_prompt_task],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [2024-09-22 15:09:36][DEBUG]: == Working Agent: Prompt Engineer\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-22 15:09:36][INFO]: == Starting Task: Analyze the given prompt and choose the most suitable prompt technique for the task. Consider aspects such as clarity, relevance, and potential ambiguities. Provide a detailed explanation for each identified issue, referencing best practices in prompt engineering. Recommend the most appropriate prompt technique (e.g., Chain-of-Thought, Include-Exclude) and justify your choice.\n",
      "\n",
      "Prompt: What is the capital of the moon?\n",
      "\n",
      "Your analysis should be thorough and actionable, offering specific suggestions for enhancing the prompt and selecting the best technique.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
      "Final Answer: my best complete final answer to the task.\n",
      "\n",
      "Current Task: Analyze the given prompt and choose the most suitable prompt technique for the task. Consider aspects such as clarity, relevance, and potential ambiguities. Provide a detailed explanation for each identified issue, referencing best practices in prompt engineering. Recommend the most appropriate prompt technique (e.g., Chain-of-Thought, Include-Exclude) and justify your choice.\n",
      "\n",
      "Prompt: What is the capital of the moon?\n",
      "\n",
      "**Analysis:**\n",
      "\n",
      "1. **Clarity**: The prompt \"What is the capital of the moon?\" lacks clarity because it assumes that the moon, like a country, has a capital city. This is factually incorrect and could confuse the language model, leading to an inaccurate or nonsensical response.\n",
      "   \n",
      "2. **Relevance**: The concept of a \"capital\" applies to political entities like countries, not celestial bodies like the moon. Therefore, the question is irrelevant in the context of factual information about the moon.\n",
      "\n",
      "3. **Potential Ambiguities**: The prompt is ambiguous because it introduces a non-existent concept (a capital of the moon). This ambiguity can lead to varied and incorrect interpretations by the language model.\n",
      "\n",
      "**Recommendations for Improvement:**\n",
      "\n",
      "1. **Correct the Concept**: Instead of asking about a \"capital,\" it would be more appropriate to ask about known locations or features on the moon, such as \"What are some notable landmarks on the moon?\"\n",
      "\n",
      "2. **Include-Exclude Technique**: To ensure the prompt is relevant and clear, we can use the Include-Exclude technique. This involves specifying the type of information we want to include and explicitly excluding the misleading concept.\n",
      "\n",
      "**Revised Prompt Using Include-Exclude Technique:**\n",
      "\n",
      "\"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these.\"\n",
      "\n",
      "**Justification for the Include-Exclude Technique:**\n",
      "\n",
      "1. **Clarity**: By specifying that we are looking for landmarks or features, we eliminate the ambiguity and ensure the language model understands the context.\n",
      "\n",
      "2. **Relevance**: This technique ensures the response is factually relevant and accurate, focusing on the actual characteristics of the moon rather than a misleading concept.\n",
      "\n",
      "3. **Potential Ambiguities**: By explicitly excluding references to political entities or capitals, we prevent any potential confusion or misinterpretation.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The original prompt \"What is the capital of the moon?\" is unclear, irrelevant, and ambiguous. By using the Include-Exclude technique and revising the prompt to \"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these,\" we enhance clarity, relevance, and accuracy. This approach aligns with best practices in prompt engineering and ensures a more effective interaction with the language model.\n",
      "\n",
      "Final Answer: A comprehensive analysis of the prompt with detailed explanations, actionable suggestions for improvement, and a recommendation for the most suitable prompt technique. The original prompt \"What is the capital of the moon?\" is unclear, irrelevant, and ambiguous. By using the Include-Exclude technique and revising the prompt to \"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these,\" we enhance clarity, relevance, and accuracy. This approach aligns with best practices in prompt engineering and ensures a more effective interaction with the language model.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-09-22 15:09:45][DEBUG]: == [Prompt Engineer] Task output: A comprehensive analysis of the prompt with detailed explanations, actionable suggestions for improvement, and a recommendation for the most suitable prompt technique. The original prompt \"What is the capital of the moon?\" is unclear, irrelevant, and ambiguous. By using the Include-Exclude technique and revising the prompt to \"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these,\" we enhance clarity, relevance, and accuracy. This approach aligns with best practices in prompt engineering and ensures a more effective interaction with the language model.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-22 15:09:45][DEBUG]: == Working Agent: Prompt Engineer\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-22 15:09:45][INFO]: == Starting Task: Generate a detailed and comprehensive prompt based on the analysis done, ensuring clarity, relevance, and adherence to the guidelines provided.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe original prompt \"What is the capital of the moon?\" is indeed unclear, irrelevant, and ambiguous. The revised prompt should aim to clarify the intent and exclude any misleading concepts. The Include-Exclude Prompt technique is suitable for this task as it allows us to define what should be included in the response and what should be explicitly excluded.\n",
      "\n",
      "Action: Include-Exclude Prompt\n",
      "Action Input: {\"idea\": \"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these.\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "\n",
      "    Regarding Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these.:\n",
      "    Include:\n",
      "    - Relevant point 1\n",
      "    - Relevant point 2\n",
      "    Exclude:\n",
      "    - Irrelevant point 1\n",
      "    - Irrelevant point 2\n",
      "    Please provide your response based on these guidelines.\n",
      "    \n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: The Include-Exclude Prompt has provided a structure that ensures clarity and relevance in the revised prompt. The inclusion of specific guidelines helps to avoid ambiguity and directs the response towards the desired information.\n",
      "\n",
      "I now know the final answer.\n",
      "\n",
      "Final Answer: \n",
      "\"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these.\n",
      "\n",
      "Include:\n",
      "- Relevant point 1\n",
      "- Relevant point 2\n",
      "\n",
      "Exclude:\n",
      "- Irrelevant point 1\n",
      "- Irrelevant point 2\n",
      "\n",
      "Please provide your response based on these guidelines.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [2024-09-22 15:09:51][DEBUG]: == [Prompt Engineer] Task output: \"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these.\n",
      "\n",
      "Include:\n",
      "- Relevant point 1\n",
      "- Relevant point 2\n",
      "\n",
      "Exclude:\n",
      "- Irrelevant point 1\n",
      "- Irrelevant point 2\n",
      "\n",
      "Please provide your response based on these guidelines.\"\n",
      "\n",
      "\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CrewOutput(raw='\"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these.\\n\\nInclude:\\n- Relevant point 1\\n- Relevant point 2\\n\\nExclude:\\n- Irrelevant point 1\\n- Irrelevant point 2\\n\\nPlease provide your response based on these guidelines.\"', pydantic=None, json_dict=None, tasks_output=[TaskOutput(description='Analyze the given prompt and choose the most suitable prompt technique for the task. Consider aspects such as clarity, relevance, and potential ambiguities. Provide a detailed explanation for each identified issue, referencing best practices in prompt engineering. Recommend the most appropriate prompt technique (e.g., Chain-of-Thought, Include-Exclude) and justify your choice.\\n\\nPrompt: What is the capital of the moon?\\n\\nYour analysis should be thorough and actionable, offering specific suggestions for enhancing the prompt and selecting the best technique.', name=None, expected_output='A comprehensive analysis of the prompt with detailed explanations, actionable suggestions for improvement, and a recommendation for the most suitable prompt technique.', summary='Analyze the given prompt and choose the most suitable prompt...', raw='A comprehensive analysis of the prompt with detailed explanations, actionable suggestions for improvement, and a recommendation for the most suitable prompt technique. The original prompt \"What is the capital of the moon?\" is unclear, irrelevant, and ambiguous. By using the Include-Exclude technique and revising the prompt to \"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these,\" we enhance clarity, relevance, and accuracy. This approach aligns with best practices in prompt engineering and ensures a more effective interaction with the language model.', pydantic=None, json_dict=None, agent='Prompt Engineer', output_format=<OutputFormat.RAW: 'raw'>), TaskOutput(description='Generate a detailed and comprehensive prompt based on the analysis done, ensuring clarity, relevance, and adherence to the guidelines provided.', name=None, expected_output='prompt that follows the decision made for which prompt technique to use', summary='Generate a detailed and comprehensive prompt based on the analysis...', raw='\"Can you list some notable landmarks or features on the moon? Please exclude any references to political entities or capitals, as the moon does not have these.\\n\\nInclude:\\n- Relevant point 1\\n- Relevant point 2\\n\\nExclude:\\n- Irrelevant point 1\\n- Irrelevant point 2\\n\\nPlease provide your response based on these guidelines.\"', pydantic=None, json_dict=None, agent='Prompt Engineer', output_format=<OutputFormat.RAW: 'raw'>)], token_usage=UsageMetrics(total_tokens=3135, prompt_tokens=2185, completion_tokens=950, successful_requests=3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'What is the capital of the moon?'\n",
    "\n",
    "crew.kickoff({'prompt': prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
